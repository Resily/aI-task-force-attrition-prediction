---
title: "心理測定データの前処理"
format: html
execute:
  echo: true
  warning: false
---

```{python}
# 必要なライブラリのインポート
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures
import lightgbm as lgb
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# データの読み込み
file_path = '/Users/snakashima/develop/aiTaskForce/aI-task-force-attrition-prediction/src/data/processed/train/学習データ_label_conversion_3_過学習False.csv'
df = pd.read_csv(file_path)

# データの基本情報を確認
print(f"データサイズ: {df.shape}")
df.head()
```

## 1. データの基本理解

```{python}
# カラム情報の確認
print("列数:", len(df.columns))
print("\n最初の10列:", df.columns[:10].tolist())

# ターゲット変数の確認（this might need adjustment based on your actual target column name）
target_col = [col for col in df.columns if 'voluntary' in col.lower() or 'turnover' in col.lower() or 'label' in col.lower()]
if target_col:
    print(f"\nターゲット変数候補: {target_col}")
    # 最初の候補をターゲット変数として使用
    target_col = target_col[0]
    print(f"使用するターゲット変数: {target_col}")
    print("クラス分布:")
    print(df[target_col].value_counts())
    print("クラス割合:")
    print(df[target_col].value_counts(normalize=True))
else:
    print("\nターゲット変数が見つかりません。適切な列名を指定してください")
    target_col = "is_voluntary_turnover"  # 仮のターゲット列名

# データ型の確認
print("\nデータ型の概要:")
print(df.dtypes.value_counts())
```

## 2. 必要最小限の前処理

### A. 欠損値処理
```{python}
# 心理測定データは欠損が少ないはずですが
print("欠損値の確認:")
null_counts = df.isnull().sum()
print(null_counts[null_counts > 0])

# 全体の欠損率
total_nulls = df.isnull().sum().sum()
total_values = df.shape[0] * df.shape[1]
print(f"\n全体の欠損率: {100 * total_nulls / total_values:.2f}%")

# 欠損があれば中央値補完（平均値ではなく）
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')

# ターゲット変数とそれ以外を分割
y = df[target_col]
X = df.drop(columns=[target_col])

# 欠損値がある場合のみ補完
if X.isnull().values.any():
    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
    print("\n欠損値を中央値で補完しました")
    print("補完後の欠損値数:", X.isnull().sum().sum())
```

### B. 標準化
```{python}
# EQ、コンピテンシー、ストレススコアは尺度が異なる可能性
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 標準化前後の比較を確認
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
comparison = pd.DataFrame({
    '標準化前_平均': X.mean(),
    '標準化前_標準偏差': X.std(),
    '標準化後_平均': X_scaled_df.mean(),
    '標準化後_標準偏差': X_scaled_df.std()
})
comparison.head()
```

### C. 特徴量選択（重要）
```{python}
# 40特徴量は537行に対して多すぎる
# 過学習リスクが高い

# 相関が高い特徴量を除去
correlation_matrix = df.corr()
high_corr = np.where(np.abs(correlation_matrix) > 0.9)
high_corr_pairs = [(correlation_matrix.columns[x], correlation_matrix.columns[y]) 
                   for x, y in zip(*high_corr) if x != y]
print("高相関ペア（>0.9）:")
for pair in high_corr_pairs[:10]:  # 最初の10ペアのみ表示
    print(f"{pair[0]} - {pair[1]}: {correlation_matrix.loc[pair[0], pair[1]]:.3f}")

# もしくはLasso回帰で自動選択
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV

selector = SelectFromModel(LassoCV(cv=5, random_state=42))
X_selected = selector.fit_transform(X_scaled, y)

# 選択された特徴量の確認
selected_features = X.columns[selector.get_support()]
print(f"\n選択された特徴量: {len(selected_features)}/{len(X.columns)}")
print(selected_features.tolist())
```

## 3. 小データセット専用の手法

### A. クロスバリデーション強化
```{python}
# 5-fold → 10-fold に変更
from sklearn.model_selection import StratifiedKFold
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
print(f"交差検証: {cv.n_splits}分割StratifiedKFold")
```

### B. アンサンブル学習
```{python}
# 単一モデルでは不安定
# 軽量なモデルを複数組み合わせ
from sklearn.ensemble import VotingClassifier

# LightGBMのラッパー関数
class LightGBMClassifier(lgb.LGBMClassifier):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def predict_proba(self, X):
        # VotingClassifierと互換性を持たせるためのメソッド
        return super().predict_proba(X)

models = [
    ('lgb', LightGBMClassifier(num_leaves=10)),  # 軽量設定
    ('xgb', xgb.XGBClassifier(max_depth=3)),
    ('lr', LogisticRegression(C=0.1))
]

ensemble = VotingClassifier(models, voting='soft')
print("アンサンブルモデル構成:")
for name, model in models:
    print(f"- {name}: {type(model).__name__}")
```

## 4. 心理測定データ特有の処理

### A. ドメイン知識活用
```{python}
# EQ総合スコアを作成
eq_cols = [col for col in X.columns if 'EQ能力_C0' in col and 'スコア' in col]

if len(eq_cols) >= 4:
    print("EQ関連のカラム:", eq_cols)
    df['EQ_total'] = df[eq_cols].mean(axis=1)
    print("EQ総合スコアを作成しました")
    print(f"EQ総合スコア平均: {df['EQ_total'].mean():.2f}, 標準偏差: {df['EQ_total'].std():.2f}")

# ストレスリスク総合スコア
stress_risk_cols = [col for col in df.columns if 'リスクパターン' in col]

if len(stress_risk_cols) > 0:
    print("\nストレスリスク関連カラム:", stress_risk_cols)
    df['stress_risk_total'] = df[stress_risk_cols].mean(axis=1)
    print("ストレスリスク総合スコアを作成しました")
    print(f"ストレスリスク総合スコア平均: {df['stress_risk_total'].mean():.2f}, 標準偏差: {df['stress_risk_total'].std():.2f}")
```

### B. 非線形関係の捕捉
```{python}
# 心理指標は相互作用が重要
from sklearn.preprocessing import PolynomialFeatures

# 2次交互作用項（少しだけ）
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)

# 計算量が多すぎるので、選択された特徴量のみで交互作用を生成
X_selected_df = pd.DataFrame(X_scaled[:, selector.get_support()], columns=selected_features)

# 特徴量が多すぎる場合は上位5つだけで交互作用を計算
if len(X_selected_df.columns) > 5:
    print(f"特徴量が{len(X_selected_df.columns)}個あるため、上位5つのみで交互作用を計算します")
    X_top5 = X_selected_df.iloc[:, :5]
    X_poly = poly.fit_transform(X_top5)
    poly_feature_names = poly.get_feature_names_out(X_top5.columns)
else:
    X_poly = poly.fit_transform(X_selected_df)
    poly_feature_names = poly.get_feature_names_out(X_selected_df.columns)

print(f"交互作用生成後の特徴量数: {len(poly_feature_names)}")
print("最初の10個の交互作用特徴量:")
print(poly_feature_names[:10])

# ただし特徴量爆発に注意
print(f"\n注意: 交互作用項を含めると元の{len(X_selected_df.columns)}個の特徴量から{len(poly_feature_names)}個に増加します")
print("モデルの複雑さと過学習のバランスに注意が必要です")
```

## 5. 最終推奨戦略

```{python}
# 1. 基本前処理のみ
X_processed = StandardScaler().fit_transform(X)

# 2. 特徴量選択で次元削減
X_selected = SelectFromModel(LassoCV()).fit_transform(X_processed, y)

# 3. class_weight調整
lgb_model = LightGBMClassifier(
    class_weight='balanced',
    num_leaves=10,  # 軽量設定
    max_depth=3,
    random_state=42
)

# 4. 10-fold CV
scores = cross_val_score(lgb_model, X_selected, y, 
                        cv=StratifiedKFold(10), 
                        scoring='average_precision')  # PR-AUC

print(f"平均PR-AUCスコア: {scores.mean():.4f}, 標準偏差: {scores.std():.4f}")
```

## 6. 個別実装例：EQ総合スコアとストレスリスク

```{python}
# EQ総合スコアを作成（シンプル実装）
df['EQ_total'] = (df['EQ能力_C01_感情の識別_スコア'] + 
                  df['EQ能力_C02_感情の利用_スコア'] + 
                  df['EQ能力_C03_感情の理解_スコア'] + 
                  df['EQ能力_C04_感情の調整_スコア']) / 4

# ストレスリスク総合スコア（シンプル実装）
stress_risk_cols = [col for col in df.columns if 'リスクパターン' in col]
df['stress_risk_total'] = df[stress_risk_cols].mean(axis=1)
```

## 7. 加工済みデータの保存

```{python}
# 加工済みデータを保存する
processed_file_path = '/Users/snakashima/develop/aiTaskForce/aI-task-force-attrition-prediction/src/data/processed/train/学習データ_label_conversion_3_前処理済み.csv'

# 重要な特徴量とターゲットのみ保存
df_processed = pd.DataFrame(X_selected, columns=selected_features)
df_processed[target_col] = y.values

print(f"前処理済みデータ（特徴量選択後）のサイズ: {df_processed.shape}")
print(f"保存パス: {processed_file_path}")

# 実際に保存する場合はコメントを外す
# df_processed.to_csv(processed_file_path, index=False)
```

## 8. 結果の可視化

```{python}
# 選択された特徴量の重要度可視化
if hasattr(selector.estimator_, 'coef_'):
    feature_importance = pd.DataFrame({
        '特徴量': selected_features,
        '重要度': np.abs(selector.estimator_.coef_)
    }).sort_values(by='重要度', ascending=False)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x='重要度', y='特徴量', data=feature_importance.head(15))
    plt.title('Lasso回帰による特徴量重要度（上位15）')
    plt.tight_layout()
    plt.show()
```

## 9. RandomForestによる予測

```{python}
# 必要なライブラリのインポート
from sklearn.ensemble import RandomForestClassifier
import optuna
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

# Optunaによるパラメータ最適化
def objective(trial):
    # パラメータ探索空間の定義
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 3, 10)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)
    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])
    class_weight = trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])
    
    # モデルの構築
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features,
        class_weight=class_weight,
        random_state=42,
        n_jobs=-1  # 全てのCPUを使用
    )
    
    # クロスバリデーションでスコア計算
    score = cross_val_score(
        rf, X_selected, y, 
        cv=StratifiedKFold(10, shuffle=True, random_state=42),
        scoring='average_precision'
    ).mean()
    
    return score

# 最適化の実行（試行回数は少なめに設定、本番ではもっと増やす）
print("Optunaによるパラメータ最適化を開始...")
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)  # 試行回数を増やすとより良い結果が得られる可能性があります

# 最適パラメータの表示
print("最適パラメータ:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")
print(f"最高スコア (PR-AUC): {study.best_value:.4f}")

# 最適パラメータでモデル構築
best_rf = RandomForestClassifier(
    **study.best_params,
    random_state=42,
    n_jobs=-1
)

# 再度評価
rf_scores = cross_val_score(
    best_rf, X_selected, y,
    cv=StratifiedKFold(10, shuffle=True, random_state=42),
    scoring='average_precision'
)
print(f"RandomForest 10-fold CV スコア (PR-AUC): {rf_scores.mean():.4f} ± {rf_scores.std():.4f}")

# LightGBMとの比較
print("\nモデル比較:")
print(f"LightGBM: {scores.mean():.4f} ± {scores.std():.4f}")
print(f"RandomForest: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}")

# パラメータ最適化の履歴を可視化
plt.figure(figsize=(10, 6))
optuna.visualization.matplotlib.plot_optimization_history(study)
plt.title('RandomForest パラメータ最適化の履歴')
plt.show()

# パラメータの重要度を可視化
plt.figure(figsize=(10, 6))
optuna.visualization.matplotlib.plot_param_importances(study)
plt.title('RandomForest パラメータの重要度')
plt.tight_layout()
plt.show()

# RandomForestの特徴量重要度を表示
best_rf.fit(X_selected, y)
feature_importances = pd.DataFrame({
    '特徴量': selected_features,
    '重要度': best_rf.feature_importances_
}).sort_values(by='重要度', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='重要度', y='特徴量', data=feature_importances.head(15))
plt.title('RandomForestによる特徴量重要度（上位15）')
plt.tight_layout()
plt.show()
```

## 10. モデルの比較と最終選択

```{python}
# 各モデルの評価結果をまとめる
models_results = pd.DataFrame({
    'モデル': ['LightGBM', 'RandomForest', 'アンサンブル'],
    'PR-AUC (平均)': [scores.mean(), rf_scores.mean(), np.nan],  # アンサンブルはまだ評価していない
    'PR-AUC (標準偏差)': [scores.std(), rf_scores.std(), np.nan]
})

print("モデル評価結果の比較:")
models_results

# PR-AUCの比較をグラフ化
plt.figure(figsize=(10, 6))
colors = ['skyblue', 'lightgreen', 'lightcoral']
bar_plot = plt.bar(
    models_results['モデル'][:2],  # アンサンブルはまだ計算していないので除外
    models_results['PR-AUC (平均)'][:2],
    yerr=models_results['PR-AUC (標準偏差)'][:2],
    color=colors[:2],
    capsize=10
)

for i, bar in enumerate(bar_plot):
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width()/2.,
        height + 0.005,
        f'{models_results["PR-AUC (平均)"][i]:.4f}',
        ha='center', va='bottom'
    )

plt.ylim(0.5, max(1, models_results['PR-AUC (平均)'].max() + 0.1))
plt.title('各モデルのPR-AUCスコア比較')
plt.ylabel('PR-AUC')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 最終モデルの選択
best_model_idx = models_results['PR-AUC (平均)'][:2].idxmax()
best_model_name = models_results.loc[best_model_idx, 'モデル']
print(f"最も性能の良いモデル: {best_model_name} (PR-AUC: {models_results.loc[best_model_idx, 'PR-AUC (平均)']:.4f})")
```

## 11. 最終モデルの保存

```{python}
# 最終モデルを選択して保存（LightGBMかRandomForestで良い方）
import joblib
import os

# 保存先ディレクトリの確認と作成
model_dir = '/Users/snakashima/develop/aiTaskForce/aI-task-force-attrition-prediction/src/model/final'
os.makedirs(model_dir, exist_ok=True)

# 最終モデルの学習（全データを使用）
if best_model_name == 'LightGBM':
    final_model = lgb_model
    final_model.fit(X_selected, y)
    model_path = os.path.join(model_dir, 'final_lightgbm_model.pkl')
else:  # RandomForest
    final_model = best_rf
    final_model.fit(X_selected, y)
    model_path = os.path.join(model_dir, 'final_randomforest_model.pkl')

# モデルの保存
joblib.dump(final_model, model_path)
print(f"最終モデルを保存しました: {model_path}")

# スケーラーと特徴量セレクタも保存（予測時に必要）
scaler_path = os.path.join(model_dir, 'scaler.pkl')
selector_path = os.path.join(model_dir, 'feature_selector.pkl')

joblib.dump(scaler, scaler_path)
joblib.dump(selector, selector_path)
print(f"スケーラーとセレクタを保存しました: {scaler_path}, {selector_path}")

# 選択された特徴量のリストも保存
with open(os.path.join(model_dir, 'selected_features.txt'), 'w') as f:
    f.write('\n'.join(selected_features))
print(f"選択された特徴量リストを保存しました")

print("\n最終モデル情報:")
print(f"モデル: {best_model_name}")
print(f"特徴量数: {len(selected_features)}")
print(f"PR-AUC: {models_results.loc[best_model_idx, 'PR-AUC (平均)']:.4f} ± {models_results.loc[best_model_idx, 'PR-AUC (標準偏差)']:.4f}")
```